# -*- coding: utf-8 -*-
"""
    #-------------------------------------------------------------------------------
    # Name:        PARSE ARXIV XML and build metadata_plus index
    # Purpose:     Parses the XML metadata file from ArXiv, and inserts some
    #              of the fields into Solr for each record. It also gets the
    #              dblp url from the metadats index.
    #
    # Author:      Ashwath Sampath
    #
    # Created:     16-12-2018
    # Copyright:   (c) Ashwath Sampath 2018
    #-------------------------------------------------------------------------------

"""
from collections import defaultdict
from lxml import etree
import pysolr
import datetime
import requests
from time import time

# Parse the Arxiv xml file
def get_xml_root():
    """ Gets the root of the arxiv xml tree and returns it."""
    xml_filepath = '/home/ashwath/Files/arxiv-cs-all-until201712031.xml'
#    xml_filepath = 'D:\\Coursework\\HiWi\\arxiv-cs-all-until201712031.xml'
    doc = etree.parse(xml_filepath)
    root = doc.getroot()
    return root

def search_solr(query, collection, search_field, num_rows):
    """ Searches the specified collection on the specified search_field (and a
    specified no. of rows) and fetches and retuens results using parse_json"""
    solr_url = 'http://localhost:8983/solr/' + collection + '/select'
    # Exact search only
    query = '"' + query + '"'
    url_params = {'q': query, 'rows': num_rows, 'df': search_field}
    solr_response = requests.get(solr_url, params=url_params)
    if solr_response.ok:
        data = solr_response.json()
        return parse_json(data, collection)
    else:
        print("Invalid response returned from Solr")
        sys.exit(11)


def parse_json(data, collection):
    """ Calls the appropriate json parser based on the collection,
    returns whatever the parser returns, along with the query and
    num_responses, which it gets from the json response. If there
    are no results, it returns ([], query, 0)"""
    # FIRST (only applies to references_plus index), check if this
    # annotation is already present
    if collection == 'references_plus':
        return parse_references_plus_json(data)
    # query is the actual phrase searched in Solr
    query = data['responseHeader']['params']['q']
    num_responses = data['response']['numFound']
    if num_responses == 0:
        # Return None if not found.
        return None
    results = parse_metadata_json(data)
    return results

def parse_metadata_json(data):
    """ Function to parse the json response from the metadata or the
    arxiv_metadata collections in Solr. It returns the dblp url. 
    docs is a list of one result, so this is obtained by docs[0].get('url')"""
    # docs contains authors, title, id generated by Solr, url
    docs = data['response']['docs']
    # NOTE: there are records without authors and urls. This is why the
    # get method is always used to get the value instead of getting the value
    # by applying the [] operator on the key.
    if docs == []:
        # Return None if not found.
        return None
    dblp_url = docs[0].get('url') 
    return dblp_url


def parse_xml_insert_into_solr(root):
    """ Function which parses the arxiv xml, and inserts some of the metadata
    into an index in Apache Solr."""
    # Set the 2 namespaces which are used in the xml file: Open archive,
    # and Dublin Core.
    solr = pysolr.Solr('http://localhost:8983/solr/metadata_plus')
    namespace = {'dc': 'http://purl.org/dc/elements/1.1/',
                 'oai_dc': 'http://www.openarchives.org/OAI/2.0/oai_dc/'}
    # NOTE: this is the fully qualified version of descending through the ns
    # for m in root.find('./record/metadata/'
    # '{http://www.openarchives.org/OAI/2.0/oai_dc/}dc/'
    #'[{http://purl.org/dc/elements/1.1/}identifier=
    # "http://arxiv.org/abs/0704.0002"]'):
    
    # Descend down to the metadata node in all the records: it has all the
    # interesting fields as its children.
    metadata_xpath = './record/metadata/'
    # metadata is a list, but len(metadata) = len(root) = 155308.
    # Each member of metadata, metadata[i] is of type lxml.etree._Element node,
    # the same as root. So we can loop through it to get its children.
    metadata = root.findall(metadata_xpath, namespaces=namespace)
    #print(len(metadata))
    # Get the index of the first character after the dc prefix, it's going to
    # be the same for all children. {http://purl.org/dc/elements/1.1/}title
    # Get the tag of the first record's title, and run find on that. Ans. 34
    tag_with_prefix = metadata[0][0].tag # for first child
    start_index = tag_with_prefix.find('}') + 1 # 34
    list_for_solr = []
    for metadata_element in metadata:
        solr_record = {}
        #solr_record = defaultdict(list)
        published_dates = []
        authors = []
        for child in metadata_element:
            if child.tag[start_index:] == 'title':
                solr_record['title'] = child.text
            elif child.tag[start_index:] == 'creator':
                #solr_record['authors'].append(child.text)
                authors.append(child.text)
            elif child.tag[start_index:] == 'date':
                #solr_record['published_date'].append(child.text)
                published_dates.append(child.text)
            elif child.tag[start_index:] == 'identifier' and \
              child.text.startswith('http://arxiv'):
                # this is a list
                solr_record['arxiv_url'] = child.text
                id_startindex = child.text.rfind('/') + 1
                arxiv_identifier = child.text[id_startindex:]
                solr_record['arxiv_identifier'] = arxiv_identifier  # Return only the first date (1st element in list): the date of version 1.

        # Add the dates
        if len(published_dates) == 1:
            solr_record['published_date'] = published_dates[0]
            solr_record['revision_dates'] = 'unavailable'
        else:
            solr_record['published_date'] = published_dates[0]
            revision = ';'.join([datetime.datetime.strptime(pdate[:10], '%Y-%m-%d').strftime('%B %d, %Y') for pdate in published_dates[1:]])
            solr_record['revision_dates'] = 'revised on {}'.format(revision)
        # Add the authors
        solr_record['authors'] = '; '.join(authors)
        # Get the dblp url from the metadata index
        dblp_url = search_solr(arxiv_identifier, 'metadata', 'arxiv_identifier', 1)
        dblp_url = dblp_url if dblp_url is not None else 'unavailable'
        solr_record['dblp_url'] = dblp_url
        print(arxiv_identifier)
        # Append each record in dict to the list_for_solr list.
        list_for_solr.append(solr_record)
    print(len(list_for_solr))
    solr.add(list_for_solr)

if __name__ == '__main__':
    start_time = time()
    xml_root = get_xml_root()
    parse_xml_insert_into_solr(xml_root)
    print("Completed in {} seconds!".format(time() - start_time))