# -*- coding: utf-8 -*-
"""
    #-------------------------------------------------------------------------------
    # Name:        CREATE REFERENCES PLUS
    # Purpose:     Uses indices for references, papers, metadata and arxiv-metadata
    #              and inserts data into a new index 'references_plus' which adds
    #              data from the other indices to the references index
    #
    # Author:      Ashwath Sampath
    #
    # Created:     21-10-2018
    # Copyright:   (c) Ashwath Sampath 2018
    #-------------------------------------------------------------------------------

"""
import os
import csv
from collections import defaultdict
import requests
import datetime
import pysolr
from glob import iglob
from time import time

def search_solr(query, collection, search_field, num_rows):
    """ Searches the specified collection on the specified search_field (and a
    specified no. of rows) and fetches and retuens results using parse_json"""
    solr_url = 'http://localhost:8983/solr/' + collection + '/select'
    # Exact search only
    query = '"' + query + '"'
    url_params = {'q': query, 'rows': num_rows, 'df': search_field}
    solr_response = requests.get(solr_url, params=url_params)
    if solr_response.ok:
        data = solr_response.json()
        return parse_json(data, collection)
    else:
        print("Invalid response returned from Solr")
        sys.exit(11)


def parse_json(data, collection):
    """ Calls the appropriate json parser based on the collection,
    returns whatever the parser returns, along with the query and
    num_responses, which it gets from the json response. If there
    are no results, it returns ([], query, 0)"""
    # FIRST (only applies to references_plus index), check if this
    # annotation is already present
    if collection == 'references_plus':
        return parse_references_plus_json(data)
    # query is the actual phrase searched in Solr
    query = data['responseHeader']['params']['q']
    num_responses = data['response']['numFound']
    if num_responses == 0:
        return []
    if collection == 'papers':
        results = parse_sentence_json(data)
    elif collection == 'arxiv_metadata':
        results = parse_arxiv_metadata_json(data)
    elif collection == 'metadata':
        results = parse_metadata_json(data)
    elif collection == 'references':
        results = parse_refs_json(data)
    return results

def parse_references_plus_json(data):
    """ Function to parse the json response from the references_plus collection
    in Solr (which is being built using this program). It returns the num_results
    only. As the same annotation is present in multiple refs files, we want to only
    search for it once in the papers index (we add everything in this one time). So,
    searching again will result in duplicate values getting inserted. So this returns 0 if
    it is indeed a new record, a number greater than 0 (num_responses) otherwise"""
    # docs contains sentence, fileName, id generated by Solr
    num_responses = data['response']['numFound']
    #print("NUMBER OF RESPONSES", num_responses)
    return num_responses

def parse_sentence_json(data):
    """ Function to parse the json response from the papers collection
    in Solr. It returns the results as a list with the sentence, file name
    and title."""
    # docs contains sentence, fileName, id generated by Solr
    docs = data['response']['docs']
    # Create a list object for the results with sentence, fileName and title
    results = [[docs[i].get('sentence')[0], docs[i].get('fileName'),
                docs[i].get('sentencenum')]
                for i in range(len(docs))]
    return results

def parse_arxiv_metadata_json(data):
    """ Function to parse the json response from the metadata or the
    arxiv_metadata collections in Solr. It returns the results as a
    list with the sentence, file name and title."""
    # docs contains authors, title, id generated by Solr, url
    docs = data['response']['docs']
    # NOTE: there are records without authors and urls. This is why the
    # get method is always used to get the value instead of getting the value
    # by applying the [] operator on the key.
    results = [[docs[i].get('title'), docs[i].get('authors'),
                docs[i].get('url'), docs[i].get('published_date')]
                for i in range(len(docs))]
    return results
    
def parse_metadata_json(data):
    """ Function to parse the json response from the metadata or the
    arxiv_metadata collections in Solr. It returns the dblp url. 
    docs is a list of one result, so this is obtained by docs[0].get('url')"""
    # docs contains authors, title, id generated by Solr, url
    docs = data['response']['docs']
    # NOTE: there are records without authors and urls. This is why the
    # get method is always used to get the value instead of getting the value
    # by applying the [] operator on the key.

    dblp_url = docs[0].get('url') 
    return dblp_url

def parse_file_build_records():
    """ Read each of the refs files, which have annotations with their associated details (cited papers)
    Go through each annotation, details pair in this file, check if the  annotation is already in the
    index. If yes, continue to the next line without doing anything. If no, search the papers index
    for the annotation. Get the sentence and the citing paper's arxiv identifier. This is then used to
    query the 2 metadata indices and get the relevant fields, which are added to the new Solr index.
    Solr field definition for new Solr index references_plus:

    <!-- REFS file fields: cited paper-->
    <field name="annotation" type="string" indexed="true" stored="true" multiValued="false"/> 
    <field name="cited_paper_details" type="text_classic" indexed="true" stored="true" multiValued="false"/> 
    <!-- Insert this only for debugging, to search for records from a particular file-->
    <field name="reference_filename" type="string" indexed="false" stored="true" multiValued="false"/> 

    <!-- Citing paper fields: papers, metadata, arxiv_metadata -->
    <!-- Papers -->
    <field name="citing_sentencenum" type="pint" indexed="true" stored="true" multiValued="false"/>
    <field name="citing_sentence" type="text_classic" indexed="true" stored="true" multiValued="false"/>
    <field name="citing_arxiv_identifier" type="string" indexed="true" stored="true" multiValued="false"/>
    
    <!-- arxiv metadata-->
    <field name="citing_arxiv_url" type="string" indexed="true" stored="true" multiValued="false"/> 
    <field name="citing_paper_authors" type="text_classic" indexed="true" stored="true" multiValued="false"/> 
    <field name="citing_paper_title" type="text_classic" indexed="true" stored="true" multiValued="false"/> 
    <field name="citing_published_date" type="daterange" indexed="true" stored="true" multiValued="false"/>
    <field name="citing_revision_dates" type="string" indexed="true" stored="true" multiValued="false"/>

    <!-- meta field: dblp_url-->
    <field name="citing_dblp_url" type="string" indexed="true" stored="true" multiValued="false"/> 


     """
    # Make a connection to Solr
    solr = pysolr.Solr('http://localhost:8983/solr/references_plus')
    folderpath = '/home/ashwath/Files/arxiv-cs-dataset-LREC2018/'
    file_no = 0
    for filepath in iglob(os.path.join(folderpath, '*.refs')):
        with open(filepath, 'r') as file:
            filename = os.path.basename(filepath)
            file_no += 1
            print(filename, file_no)
            filename_without_extension = '.'.join(filename.split('.')[:2])
            # Initialize list_for_solr: I insert once into Solr for the results from one file
            list_for_solr = []
            # Create a CSV reader for the current file, the fields are inserted in a list
            # [annotation, details, emtpyfiled] for a line annotation;details;
            csv_reader = csv.reader(file, delimiter=';')
            for record in csv_reader:
            #for annotation, details, dummy in csv_reader:
                # Go through each annotation, details pair in this file, check if the
                # annotation is already in the index. If yes, continue to the next line
                # without doing anything. If no, search the papers index for the annotation.
                # Get the sentence and the citing paper's arxiv identifier. This is then used
                # to query the 2 metadata indices and get the relevant fields, which are added to
                # the final index. 
                # num_results returned for references_plus. If it is >0, don't do anything. If it is
                # 0, add the solr record. (Note, the num_rows argument is set to a value higher than
                # 0 : can also be 1)
                try:
                    # Lines which do not have 2 semicolons are skipped -- they will not be indexed.
                    annotation = record[0]
                    # There is some very bad data like just GC or DBLP. Do not process this, skip to next
                    # record. This will produce millions of extra rows from papers
                    if annotation in ('GC', 'GC:', "DBLP", 'DBLP:', 'dblp', 'dblp:'):
                        continue
                    details = record[1]
                    dummy = record[2]
                except IndexError:
                    # Just go to the next line
                    continue

                if search_solr(annotation, 'references_plus', 'annotation', 1) == 0:
                    # papers_result is a list of lists with 3 fields in each sublist: sentence (string),
                    # arxiv_identifier (string), sentencenum (integer).

                    papers_result = search_solr(annotation, 'papers', 'sentence', 100000)
                    
                    #print(len(papers_result))
                    for sentence, arxiv_identifier, sentencenum in papers_result:
                        # IF NO RESULTS ARE FOUND, paper_result = [] and this loop is not entered. 
                        # It goes to the next line in the outer for loop (annotation, details)
                        solr_record = {}
                        # NOTE: Annotations in the refs files don't have < and >
                        solr_record['annotation'] = '<{}>'.format(annotation)
                        solr_record['cited_paper_details'] = details
                        # Debug field: can be used to find records created from a particular (refs) file
                        solr_record['reference_filename'] = filename_without_extension
                        solr_record['citing_sentencenum'] = sentencenum
                        solr_record['citing_sentence'] = sentence
                        solr_record['citing_arxiv_identifier'] = arxiv_identifier
                        # arxiv_metadata_result will be a list of lists with each list containing title (string),
                        # authors (list), arxiv url (string), published_dates (list)
                        arxiv_metadata_result = search_solr(arxiv_identifier, 'arxiv_metadata', 'arxiv_identifier', 1)
                        # IMPORTANT!: None is returned if there is no dblp_url. If None, set it to 'unavailable'. This 
                        # is checked in Django and the DBLP URL is deactivated with a message if it is 'unavailable'. 
                        dblp_url = search_solr(arxiv_identifier, 'metadata', 'arxiv_identifier', 1)
                        dblp_url = dblp_url if dblp_url is not None else 'unavailable'
                        solr_record['citing_dblp_url'] = dblp_url
                        for title, authors, arxiv_url, published_dates in arxiv_metadata_result:
                            # Flatten the published dates into a single string. Not using a DateRange field because
                            # a grouping is done in django_paper_search which needs the dates to be a single string
                            if len(published_dates) == 1:
                                solr_record['citing_published_date'] = published_dates[0]
                                solr_record['citing_revision_dates'] = 'unavailable'
                            else:
                                solr_record['citing_published_date'] = published_dates[0]
                                revision = ';'.join([datetime.datetime.strptime(pdate[:10], '%Y-%m-%d').strftime('%B %d, %Y') for pdate in published_dates[1:]])
                                solr_record['citing_revision_dates'] = 'revised on {}'.format(revision)
                            #published_dates = ';'.join([datetime.datetime.strptime(pdate[:10], '%Y-%m-%d').strftime('%Y-%m-%d') for  pdate in published_dates])
                            #print(published_dates)
                            solr_record['citing_paper_title'] = title
                            solr_record['citing_paper_authors'] = '; '.join(authors)
                            solr_record['citing_arxiv_url'] = arxiv_url
                        list_for_solr.append(solr_record)
            # As the refs input data is very low quality, a check is needed to see if the same annotation occurs twice in the same file
            # All the fields are strings or numbers, there are no lists within the dictionaries. REMOVE all duplicate dictionaries (this will
            # happen if the same annotation appears twice in the same file)
            unique_sets = set(frozenset(d.items()) for d in list_for_solr)
            unique_dicts = [dict(s) for s in unique_sets]
            # Add to Solr
            solr.add(unique_dicts)
            #print("Inserted list length =", len(list_for_solr))
                

if __name__ == '__main__':
    start_time = time()
    parse_file_build_records()
    print("Completed in {} seconds!".format(time() - start_time))
    
